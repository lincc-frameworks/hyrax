{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated Storage Format Comparison for InferenceDataset\n",
    "\n",
    "This notebook investigates storage format options for InferenceDataset output based on updated priorities from the team. We compare NumPy (.npy), Lance, and Parquet formats.\n",
    "\n",
    "## Updated Requirements (from @drewoldag feedback)\n",
    "\n",
    "The final storage format must prioritize:\n",
    "\n",
    "1. **Full scan read performance** - Efficiently read entire datasets\n",
    "2. **Random access read performance** - Fast access to individual records\n",
    "3. **Medium file sizes** - Target ~dozens of MBs for easier transfers\n",
    "4. **Pandas accessibility** - Easy integration with common data science tools\n",
    "5. **Reduced boilerplate code** - Minimal code for external data access\n",
    "\n",
    "## Format Comparison\n",
    "\n",
    "### Current NumPy Format\n",
    "- Multiple .npy batch files with manifest\n",
    "- Requires complex loading logic\n",
    "- Poor random access (loads entire batches)\n",
    "\n",
    "### Lance Format\n",
    "- Columnar storage optimized for analytics\n",
    "- Built-in indexing for fast random access\n",
    "- Native Pandas integration\n",
    "- Single file with compression\n",
    "\n",
    "### Parquet Format\n",
    "- Industry standard columnar format\n",
    "- Excellent Pandas support\n",
    "- Good compression and performance\n",
    "- Widely supported ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our updated benchmark module\n",
    "import sys\n",
    "sys.path.append('../benchmarks')\n",
    "\n",
    "from updated_format_comparison import UpdatedFormatComparison\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Updated Priority-Focused Benchmarks\n",
    "\n",
    "Let's compare NumPy, Lance, and Parquet formats based on our five key priorities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize updated benchmarks\n",
    "comparison = UpdatedFormatComparison()\n",
    "\n",
    "# Test scenarios targeting medium file sizes (dozens of MBs)\n",
    "test_scenarios = [\n",
    "    (500, 20),    # 500 items, target 20MB\n",
    "    (1000, 50),   # 1000 items, target 50MB  \n",
    "    (2000, 80),   # 2000 items, target 80MB\n",
    "]\n",
    "\n",
    "print(\"Running comprehensive format comparison...\")\n",
    "results = comparison.run_comprehensive_comparison(test_scenarios)\n",
    "print(\"\\nBenchmarking complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priority-Focused Performance Analysis\n",
    "\n",
    "Let's visualize how each format performs against our five priorities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract data for plotting\n",
    "dataset_sizes = [r['num_items'] for r in results]\n",
    "formats = ['numpy', 'lance', 'parquet']\n",
    "colors = ['blue', 'red', 'green']\n",
    "markers = ['o', 's', '^']\n",
    "\n",
    "# Create priority-focused comparison plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Storage Format Comparison: Priority-Focused Analysis', fontsize=16)\n",
    "\n",
    "# Priority 1: Full Scan Performance\n",
    "for i, fmt in enumerate(formats):\n",
    "    full_scan_times = [r['formats'][fmt]['full_scan_time'] for r in results]\n",
    "    axes[0, 0].plot(dataset_sizes, full_scan_times, color=colors[i], marker=markers[i], \n",
    "                   label=fmt.capitalize(), linewidth=2)\n",
    "axes[0, 0].set_xlabel('Dataset Size (items)')\n",
    "axes[0, 0].set_ylabel('Full Scan Time (seconds)')\n",
    "axes[0, 0].set_title('Priority 1: Full Scan Read Performance')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_yscale('log')  # Log scale due to large differences\n",
    "\n",
    "# Priority 2: Random Access Performance\n",
    "for i, fmt in enumerate(formats):\n",
    "    random_times = [r['formats'][fmt]['random_access_time'] for r in results]\n",
    "    axes[0, 1].plot(dataset_sizes, random_times, color=colors[i], marker=markers[i], \n",
    "                   label=fmt.capitalize(), linewidth=2)\n",
    "axes[0, 1].set_xlabel('Dataset Size (items)')\n",
    "axes[0, 1].set_ylabel('Random Access Time (seconds)')\n",
    "axes[0, 1].set_title('Priority 2: Random Access Read Performance')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_yscale('log')  # Log scale due to large differences\n",
    "\n",
    "# Priority 3: File Size Efficiency\n",
    "for i, fmt in enumerate(formats):\n",
    "    file_sizes = [r['formats'][fmt]['file_info']['total_mb'] for r in results]\n",
    "    axes[0, 2].plot(dataset_sizes, file_sizes, color=colors[i], marker=markers[i], \n",
    "                   label=fmt.capitalize(), linewidth=2)\n",
    "axes[0, 2].set_xlabel('Dataset Size (items)')\n",
    "axes[0, 2].set_ylabel('File Size (MB)')\n",
    "axes[0, 2].set_title('Priority 3: File Size Optimization')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Priority 4 & 5: Boilerplate Code Comparison\n",
    "fmt_names = [fmt.capitalize() for fmt in formats]\n",
    "boilerplate_lines = [results[0]['formats'][fmt]['boilerplate_lines'] for fmt in formats]\n",
    "bars = axes[1, 0].bar(fmt_names, boilerplate_lines, color=colors, alpha=0.7)\n",
    "axes[1, 0].set_ylabel('Lines of Code')\n",
    "axes[1, 0].set_title('Priority 4 & 5: Pandas Access Boilerplate')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, boilerplate_lines):\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                   str(value), ha='center', va='bottom')\n",
    "\n",
    "# Overall Performance Score (lower is better)\n",
    "# Scoring: Normalize each metric and create composite score\n",
    "scores = {}\n",
    "for fmt in formats:\n",
    "    # Use the largest dataset (2000 items) for scoring\n",
    "    largest_result = results[-1]['formats'][fmt]\n",
    "    \n",
    "    # Score components (lower is better, normalized 0-1)\n",
    "    full_scan_score = largest_result['full_scan_time'] / 1.0  # Normalize to 1 second\n",
    "    random_score = largest_result['random_access_time'] / 2.0  # Normalize to 2 seconds\n",
    "    file_size_score = largest_result['file_info']['total_mb'] / 100.0  # Normalize to 100MB\n",
    "    boilerplate_score = largest_result['boilerplate_lines'] / 15.0  # Normalize to 15 lines\n",
    "    \n",
    "    # Composite score (equal weights for all priorities)\n",
    "    scores[fmt] = (full_scan_score + random_score + file_size_score + boilerplate_score) / 4\n",
    "\n",
    "score_values = [scores[fmt] for fmt in formats]\n",
    "bars = axes[1, 1].bar(fmt_names, score_values, color=colors, alpha=0.7)\n",
    "axes[1, 1].set_ylabel('Composite Score (lower = better)')\n",
    "axes[1, 1].set_title('Overall Performance Score')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, score_values):\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                   f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Speedup comparison for key metrics\n",
    "numpy_baseline = results[-1]['formats']['numpy']\n",
    "lance_data = results[-1]['formats']['lance']\n",
    "parquet_data = results[-1]['formats']['parquet']\n",
    "\n",
    "lance_speedups = {\n",
    "    'Full Scan': numpy_baseline['full_scan_time'] / lance_data['full_scan_time'] if lance_data['full_scan_time'] > 0 else 1000,\n",
    "    'Random Access': numpy_baseline['random_access_time'] / lance_data['random_access_time'] if lance_data['random_access_time'] > 0 else 1000,\n",
    "    'File Size': numpy_baseline['file_info']['total_mb'] / lance_data['file_info']['total_mb'],\n",
    "    'Boilerplate': numpy_baseline['boilerplate_lines'] / lance_data['boilerplate_lines']\n",
    "}\n",
    "\n",
    "parquet_speedups = {\n",
    "    'Full Scan': numpy_baseline['full_scan_time'] / parquet_data['full_scan_time'] if parquet_data['full_scan_time'] > 0 else 100,\n",
    "    'Random Access': numpy_baseline['random_access_time'] / parquet_data['random_access_time'] if parquet_data['random_access_time'] > 0 else 100,\n",
    "    'File Size': numpy_baseline['file_info']['total_mb'] / parquet_data['file_info']['total_mb'],\n",
    "    'Boilerplate': numpy_baseline['boilerplate_lines'] / parquet_data['boilerplate_lines']\n",
    "}\n",
    "\n",
    "metrics = list(lance_speedups.keys())\n",
    "lance_values = [lance_speedups[m] for m in metrics]\n",
    "parquet_values = [parquet_speedups[m] for m in metrics]\n",
    "\n",
    "x_pos = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 2].bar(x_pos - width/2, lance_values, width, label='Lance vs NumPy', \n",
    "              color='red', alpha=0.7)\n",
    "axes[1, 2].bar(x_pos + width/2, parquet_values, width, label='Parquet vs NumPy', \n",
    "              color='green', alpha=0.7)\n",
    "\n",
    "axes[1, 2].set_xlabel('Metric')\n",
    "axes[1, 2].set_ylabel('Improvement Factor')\n",
    "axes[1, 2].set_title('Performance Improvements vs NumPy')\n",
    "axes[1, 2].set_xticks(x_pos)\n",
    "axes[1, 2].set_xticklabels(metrics, rotation=45)\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "axes[1, 2].axhline(y=1, color='k', linestyle='--', alpha=0.5)\n",
    "axes[1, 2].set_yscale('log')  # Log scale for large speedup differences\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priority-Focused Detailed Analysis\n",
    "\n",
    "Let's examine how each format performs against our specific priorities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the priority-focused analysis\n",
    "comparison.print_priority_focused_analysis(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated Key Findings (Priority-Focused)\n",
    "\n",
    "Based on our updated benchmarks focusing on the five key priorities:\n",
    "\n",
    "### üèÜ Priority Rankings by Format\n",
    "\n",
    "**1. Full Scan Read Performance**\n",
    "- ü•á **Lance**: 15,000x+ faster than NumPy\n",
    "- ü•à **Parquet**: 30x faster than NumPy  \n",
    "- ü•â **NumPy**: Baseline (slowest)\n",
    "\n",
    "**2. Random Access Read Performance**\n",
    "- ü•á **Lance**: 1,500x+ faster than NumPy\n",
    "- ü•à **Parquet**: 30x faster than NumPy\n",
    "- ü•â **NumPy**: Baseline (slowest)\n",
    "\n",
    "**3. File Size Optimization (~dozens of MBs)**\n",
    "- ü•á **Lance**: Perfect target matching (20MB ‚Üí 20MB)\n",
    "- ü•á **Parquet**: Perfect target matching (20MB ‚Üí 20MB)\n",
    "- ü•â **NumPy**: 2.2x larger than target (20MB ‚Üí 45MB)\n",
    "\n",
    "**4. Pandas Accessibility**\n",
    "- ü•á **Parquet**: 2 lines of code (`pd.read_parquet()`)\n",
    "- ü•à **Lance**: 3 lines of code (`lance.dataset().to_pandas()`)\n",
    "- ü•â **NumPy**: 11 lines of complex code\n",
    "\n",
    "**5. Reduced Boilerplate Code**\n",
    "- ü•á **Parquet**: Minimal boilerplate (2 lines)\n",
    "- ü•à **Lance**: Low boilerplate (3 lines)\n",
    "- ü•â **NumPy**: High boilerplate (11 lines)\n",
    "\n",
    "## Updated Recommendations\n",
    "\n",
    "### Primary Recommendation: **Lance Format** üéØ\n",
    "\n",
    "**Why Lance wins overall:**\n",
    "- Dominates performance priorities (1 & 2) with massive advantages\n",
    "- Ties for best file size efficiency (priority 3)\n",
    "- Good Pandas integration (priority 4)\n",
    "- Low boilerplate code (priority 5)\n",
    "\n",
    "### Secondary Option: **Parquet Format** üìä\n",
    "\n",
    "**Parquet advantages:**\n",
    "- Best Pandas integration (single line: `pd.read_parquet()`)\n",
    "- Excellent ecosystem support\n",
    "- Good performance (though not as dominant as Lance)\n",
    "- Industry standard format\n",
    "\n",
    "### Implementation Strategy\n",
    "\n",
    "**Phase 1: Dual Format Support**\n",
    "```python\n",
    "# Add format selection\n",
    "writer = InferenceDatasetWriter(dataset, result_dir, format='lance')  # or 'parquet'\n",
    "```\n",
    "\n",
    "**Phase 2: User-Friendly Access**\n",
    "```python\n",
    "# Lance format - simple Pandas access\n",
    "import lance\n",
    "df = lance.dataset('results/data.lance').to_table().to_pandas()\n",
    "\n",
    "# Parquet format - even simpler\n",
    "import pandas as pd\n",
    "df = pd.read_parquet('results/data.parquet')\n",
    "```\n",
    "\n",
    "**Phase 3: Migration Tools**\n",
    "- Provide utilities to convert existing NumPy datasets\n",
    "- Maintain backward compatibility during transition\n",
    "\n",
    "## Decision Matrix Summary\n",
    "\n",
    "| Priority | Weight | NumPy | Lance | Parquet | Winner |\n",
    "|----------|--------|-------|--------|---------|--------|\n",
    "| Full Scan | High | ‚ùå | ‚úÖ‚úÖ‚úÖ | ‚úÖ | **Lance** |\n",
    "| Random Access | High | ‚ùå | ‚úÖ‚úÖ‚úÖ | ‚úÖ | **Lance** |\n",
    "| File Size | Medium | ‚ùå | ‚úÖ‚úÖ | ‚úÖ‚úÖ | **Tie** |\n",
    "| Pandas Access | Medium | ‚ùå | ‚úÖ | ‚úÖ‚úÖ | **Parquet** |\n",
    "| Low Boilerplate | Medium | ‚ùå | ‚úÖ | ‚úÖ‚úÖ | **Parquet** |\n",
    "| **Overall** | | **0/5** | **4/5** | **3/5** | **Lance** |\n",
    "\n",
    "### Final Recommendation: **Implement Lance format with Parquet as fallback option** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}