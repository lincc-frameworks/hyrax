{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8e2fbb",
   "metadata": {},
   "source": [
    "# Using LR Schedulers\n",
    "\n",
    "The notebook demonstrates how to configure simple LR schedulers with Hyrax. A list of LR schedulers can be found in the PyTorch documentation here: https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LRScheduler.html.\n",
    "\n",
    "Hyrax config does not support chained or sequential LR schedulers; those must be directly defined in the model if desired to be used. This notebook will walk through the default LR used in Hyrax as well as defining an ExponentialLR defined in the config."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63286ee3",
   "metadata": {},
   "source": [
    "## The Default Scheduler in Hyrax\n",
    "\n",
    "When no scheduler is specified in the user-defined config or in the model, Hyrax uses an [ExponentialLR](https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html) scheduler with `gamma = 1`; this means it simply multiplies the starting learning rate by 1 on each epoch, which leads to no practical effect.\n",
    "\n",
    "We begin by setting up and training a HyraxCNN model with the CIAR10 dataset, like in the [Getting Started](<getting_started.ipynb>) notebook. For more information on this model, see there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyrax import Hyrax\n",
    "\n",
    "h = Hyrax()\n",
    "h.set_config(\"model.name\", \"HyraxCNN\")\n",
    "\n",
    "data_request_definition = {\n",
    "    \"train\": {\n",
    "        \"data\": {\n",
    "            \"dataset_class\": \"HyraxCifarDataset\",\n",
    "            \"data_location\": \"./data\",\n",
    "            \"fields\": [\"image\", \"label\"],\n",
    "            \"primary_id_field\": \"object_id\",\n",
    "        },\n",
    "    },\n",
    "    \"infer\": {\n",
    "        \"data\": {\n",
    "            \"dataset_class\": \"HyraxCifarDataset\",\n",
    "            \"data_location\": \"./data\",\n",
    "            \"fields\": [\"image\", \"object_id\"],\n",
    "            \"primary_id_field\": \"object_id\",\n",
    "            \"dataset_config\": {\n",
    "                \"use_training_data\": False,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "h.set_config(\"data_request\", data_request_definition)\n",
    "trained_model = h.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff6f807",
   "metadata": {},
   "source": [
    "## Performance of the model\n",
    "\n",
    "Assuming everything is working properly, the resulting accuracy on the test set and confusion matrix shouldn't be much different from what is in the [Getting Started](<getting_started.ipynb>) notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d905f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_results = h.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ebe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted_classes = np.argmax(inference_results, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f691f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./data/cifar-10-batches-py/test_batch\", \"rb\") as f_in:\n",
    "    test_data = pickle.load(f_in, encoding=\"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842ec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "y_true = test_data[b\"labels\"]\n",
    "y_pred = predicted_classes.tolist()\n",
    "\n",
    "correct = 0\n",
    "for t, p in zip(y_true, y_pred):\n",
    "    correct += t == p\n",
    "\n",
    "print(\"\\nAccuracy for test dataset:\", correct / len(y_true))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e52be7",
   "metadata": {},
   "source": [
    "Both the overall accuracy and the confusion matrix resemble what we see in the [Getting Started](<getting_started.ipynb>) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ffee2e",
   "metadata": {},
   "source": [
    "## Defining a Custom ExponentialLR\n",
    "\n",
    "Now let's see what happens when we define our own ExponentialLR scheduler. All schedulers accept an `optimizer` as an argument which should match the optimizer actually being used in the model. They also accept an argument called `last_epoch` which enables the scheduler to calculate learning rates properly in the evetn training is being resumed from some midpoint as opposed to beginning from the start. Fortunately, Hyrax takes care of the former for us and checkpointing handles the latter (you probably shouldn't introduce new learning rate schedulers between pausing and resuming training), so only the parameters that directly concern the scheduler are relevant.\n",
    "\n",
    "For the ExponentialLR, the only parameter aside from `optimizer` and `last_epoch` is `gamma`. This scheduler multiplies the previous epoch's learning rate by `gamma` to determine the new learning rate before beginning each epoch with the base learning rate being used on the very first epoch. We shall adjust our model to use a `gamma` of `0.9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7fd122",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = Hyrax()\n",
    "h.set_config(\"model.name\", \"HyraxCNN\")\n",
    "h.set_config(\"data_request\", data_request_definition)\n",
    "h.set_config(\"scheduler\", {\"name\": \"torch.optim.lr_scheduler.ExponentialLR\"})\n",
    "h.set_config(\"'torch.optim.lr_scheduler.ExponentialLR'\", {\"gamma\": 0.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651642ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = h.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9913f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_results = h.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52806e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = np.argmax(inference_results, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82140cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_data[b\"labels\"]\n",
    "y_pred = predicted_classes.tolist()\n",
    "\n",
    "correct = 0\n",
    "for t, p in zip(y_true, y_pred):\n",
    "    correct += t == p\n",
    "\n",
    "print(\"\\nAccuracy for test dataset:\", correct / len(y_true))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd8e663",
   "metadata": {},
   "source": [
    "The results may be a bit worse than without using the learning rate scheduler, but that is to be expected since we didn't really try doing any sort of tuning and just wanted to demonstrate the feature. We can also see the history of the learning rates in the TensorBoard to verify that the scheduler did in fact have an effect:\n",
    "\n",
    "![tensorboard_lr_history.png](tensorboard_lr_history.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyrax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
