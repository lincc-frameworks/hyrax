{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad04228",
   "metadata": {},
   "source": [
    "# Multi-modal data with a GraphQL alternative\n",
    "\n",
    "This notebook will demonstrate working with multi-modal or multi-source data. By default, Hyrax assumes only a single dataset will be used for a given model.\n",
    "\n",
    "As before, the two required configuration parameters are `config['general']['data_dir']` (to define the location of the data) and `config['data_set']['name']` (to define the dataset class that will load specific data from disk).\n",
    "\n",
    "Below we the standard approach of loading the default dataset defined in the Hyrax config using `h.prepare()`. We then print the dataset object to see the basic information about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d759d831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-22 13:33:57,687 hyrax:INFO] Runtime Config read from: /Users/drew/code/hyrax/src/hyrax/hyrax_default_config.toml\n"
     ]
    }
   ],
   "source": [
    "from hyrax import Hyrax\n",
    "\n",
    "h = Hyrax()\n",
    "\n",
    "# Set a few configs for later use\n",
    "h.config[\"train\"][\"epochs\"] = 1\n",
    "h.config[\"data_set.random_dataset\"][\"shape\"] = (1, 32, 32)\n",
    "h.config[\"data_set.random_dataset\"][\"size\"] = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61210469",
   "metadata": {},
   "source": [
    "## Attaching a iterable datasets to a model\n",
    "The following shows how to set the dataset to be the `HyraxRandomIterableDataset`.\n",
    "\n",
    "When experimentation is complete, the following can also be specified directly in a configuration .toml file like so:\n",
    "```toml\n",
    "[model_data]\n",
    "\n",
    "[model_data.rando]\n",
    "dataset_class = \"HyraxRandomIterableDataset\"\n",
    "data_directory = \"./data\"\n",
    "fields = [\"image\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908025fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.config[\"model_data\"] = {\n",
    "    \"rando_0\": {\n",
    "        \"dataset_class\": \"HyraxRandomIterableDataset\",\n",
    "        \"data_directory\": \"./data\",\n",
    "        \"fields\": [\"object_id\", \"image\"],\n",
    "        \"primary_id_field\": \"object_id\",\n",
    "    },\n",
    "    \"rando_1\": {\n",
    "        \"dataset_class\": \"HyraxRandomIterableDataset\",\n",
    "        \"data_directory\": \"./data\",\n",
    "        \"fields\": [\"object_id\", \"image\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce664770",
   "metadata": {},
   "source": [
    "## Examine the new iterable dataset\n",
    "As before, calling ``h.prepare()`` will return an instance of the ``DataProvider`` dataset.\n",
    "The ``DataProvider`` class can be thought of as a container of multiple datasets, as well as a gateway (in GraphQL terminology)\n",
    "that will send requests for specific data to the datasets it contains. Printing the dataset object will show the configuration of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32452f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-22 13:34:00,647 hyrax.prepare:INFO] Finished Prepare\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rando_0\n",
      "  Dataset class: HyraxRandomIterableDataset\n",
      "  Data directory: ./data\n",
      "  Fields: object_id, image\n",
      "rando_1\n",
      "  Dataset class: HyraxRandomIterableDataset\n",
      "  Data directory: ./data\n",
      "  Fields: object_id, image\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = h.prepare()\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11c36843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rando_0': {'index': 7,\n",
       "  'object_id': 7,\n",
       "  'image': array([[[0.39884484, 0.91513836, 0.89960617, ..., 0.95127994,\n",
       "           0.23864251, 0.4383163 ],\n",
       "          [0.6812738 , 0.28683555, 0.26445645, ..., 0.60255677,\n",
       "           0.57460254, 0.69453275],\n",
       "          [0.44086492, 0.36550325, 0.3526907 , ..., 0.10638863,\n",
       "           0.7537327 , 0.9651421 ],\n",
       "          ...,\n",
       "          [0.33532035, 0.36672413, 0.08297735, ..., 0.1615507 ,\n",
       "           0.44905108, 0.58613694],\n",
       "          [0.20847565, 0.1187771 , 0.1357665 , ..., 0.15624124,\n",
       "           0.93203765, 0.45069343],\n",
       "          [0.16738671, 0.369471  , 0.3347882 , ..., 0.21927172,\n",
       "           0.20030725, 0.32320726]]], dtype=float32),\n",
       "  'label': np.int64(1)},\n",
       " 'rando_1': {'index': 7,\n",
       "  'object_id': 7,\n",
       "  'image': array([[[0.39884484, 0.91513836, 0.89960617, ..., 0.95127994,\n",
       "           0.23864251, 0.4383163 ],\n",
       "          [0.6812738 , 0.28683555, 0.26445645, ..., 0.60255677,\n",
       "           0.57460254, 0.69453275],\n",
       "          [0.44086492, 0.36550325, 0.3526907 , ..., 0.10638863,\n",
       "           0.7537327 , 0.9651421 ],\n",
       "          ...,\n",
       "          [0.33532035, 0.36672413, 0.08297735, ..., 0.1615507 ,\n",
       "           0.44905108, 0.58613694],\n",
       "          [0.20847565, 0.1187771 , 0.1357665 , ..., 0.15624124,\n",
       "           0.93203765, 0.45069343],\n",
       "          [0.16738671, 0.369471  , 0.3347882 , ..., 0.21927172,\n",
       "           0.20030725, 0.32320726]]], dtype=float32),\n",
       "  'label': np.int64(1)}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.get_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5939262a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is iterable: True\n",
      "Is mappable: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is iterable: {ds.is_iterable()}\")  # Should return True\n",
    "print(f\"Is mappable: {ds.is_map()}\")  # Should return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655b64d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rando_0': {'index': 0, 'object_id': 0, 'image': array([[[0.08925092, 0.773956  , 0.6545715 , ..., 0.44341415,\n",
      "         0.45045954, 0.22723871],\n",
      "        [0.09213591, 0.55458474, 0.8878898 , ..., 0.7447621 ,\n",
      "         0.36664265, 0.9675097 ],\n",
      "        [0.41085035, 0.32582533, 0.90553576, ..., 0.38747835,\n",
      "         0.8980876 , 0.28832805],\n",
      "        ...,\n",
      "        [0.41836828, 0.5780172 , 0.5375471 , ..., 0.6346291 ,\n",
      "         0.9714626 , 0.41181087],\n",
      "        [0.15344363, 0.40878308, 0.8401149 , ..., 0.7874322 ,\n",
      "         0.3427019 , 0.5491443 ],\n",
      "        [0.19697303, 0.43141818, 0.5296637 , ..., 0.07205909,\n",
      "         0.8685205 , 0.84199315]]], dtype=float32), 'label': np.int64(1)}, 'rando_1': {'index': 0, 'object_id': 0, 'image': array([[[0.08925092, 0.773956  , 0.6545715 , ..., 0.44341415,\n",
      "         0.45045954, 0.22723871],\n",
      "        [0.09213591, 0.55458474, 0.8878898 , ..., 0.7447621 ,\n",
      "         0.36664265, 0.9675097 ],\n",
      "        [0.41085035, 0.32582533, 0.90553576, ..., 0.38747835,\n",
      "         0.8980876 , 0.28832805],\n",
      "        ...,\n",
      "        [0.41836828, 0.5780172 , 0.5375471 , ..., 0.6346291 ,\n",
      "         0.9714626 , 0.41181087],\n",
      "        [0.15344363, 0.40878308, 0.8401149 , ..., 0.7874322 ,\n",
      "         0.3427019 , 0.5491443 ],\n",
      "        [0.19697303, 0.43141818, 0.5296637 , ..., 0.07205909,\n",
      "         0.8685205 , 0.84199315]]], dtype=float32), 'label': np.int64(1)}}\n"
     ]
    }
   ],
   "source": [
    "a = next(iter(ds))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8959af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields from rando_0\n",
      "(1, 32, 32)\n",
      "Fields from rando_1\n",
      "(1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "samp = ds.get_sample()\n",
    "print(\"Fields from rando_0\")\n",
    "print(samp[\"rando_0\"][\"image\"].shape)\n",
    "print(\"Fields from rando_1\")\n",
    "print(samp[\"rando_1\"][\"image\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3368ea",
   "metadata": {},
   "source": [
    "## Pass the data through ``to_tensor``\n",
    "Since we have access to the model class, we can call the ``to_tensor`` method with example data.\n",
    "This allows easy checking that the output matches the expectations of the model architecture.\n",
    "\n",
    "In this example, we expect ``to_tensor`` to return a tuple of (Tensor, int), or specifically a multi-channel image and a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d86eb74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "@staticmethod\n",
    "def to_tensor(data_dict):\n",
    "    \"\"\"This function converts structured data to the input tensor we need to run\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dict : dict\n",
    "        The dictionary returned from our data source\n",
    "    \"\"\"\n",
    "    rando_0 = data_dict.get(\"rando_0\", {})\n",
    "    rando_1 = data_dict.get(\"rando_1\", {})\n",
    "\n",
    "    if \"image\" in rando_0:\n",
    "        image_0 = torch.from_numpy(rando_0[\"image\"])\n",
    "\n",
    "    if \"image\" in rando_1:\n",
    "        image_1 = torch.from_numpy(rando_1[\"image\"])\n",
    "\n",
    "    stack_dim = 0 if image_0.ndim == 3 else 1\n",
    "    image = torch.from_numpy(np.concatenate([image_0, image_1], axis=stack_dim))\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "m = h.model()\n",
    "m.to_tensor = to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e539239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type and shape of resulting image: <class 'torch.Tensor'>, torch.Size([2, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "res = m.to_tensor(samp)\n",
    "print(f\"Type and shape of resulting image: {type(res)}, {res.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c917f4",
   "metadata": {},
   "source": [
    "## Train with this model\n",
    "Now that we've seen that the ``to_tensor`` method is returning a reasonable form of data, we can train our model.\n",
    "As before, we call ``h.train()``.\n",
    "While it is quiet verbose, the initialization logging shows that the model instance is created with data from\n",
    "the ``DataProvider`` class, and that our new implementation of ``to_tensor`` is being used to manipulate\n",
    "the data from ``DataProvider`` into the a form that our model architecture accepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33d1861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-22 13:22:50,080 hyrax.models.hyrax_autoencoder:WARNING] Initializing HyraxAutoencoder with data sample: {'rando_0': {'index': 0, 'object_id': 0, 'image': array([[[0.08925092, 0.773956  , 0.6545715 , ..., 0.44341415,\n",
      "         0.45045954, 0.22723871],\n",
      "        [0.09213591, 0.55458474, 0.8878898 , ..., 0.7447621 ,\n",
      "         0.36664265, 0.9675097 ],\n",
      "        [0.41085035, 0.32582533, 0.90553576, ..., 0.38747835,\n",
      "         0.8980876 , 0.28832805],\n",
      "        ...,\n",
      "        [0.41836828, 0.5780172 , 0.5375471 , ..., 0.6346291 ,\n",
      "         0.9714626 , 0.41181087],\n",
      "        [0.15344363, 0.40878308, 0.8401149 , ..., 0.7874322 ,\n",
      "         0.3427019 , 0.5491443 ],\n",
      "        [0.19697303, 0.43141818, 0.5296637 , ..., 0.07205909,\n",
      "         0.8685205 , 0.84199315]]], dtype=float32), 'label': np.int64(1)}, 'rando_1': {'index': 0, 'object_id': 0, 'image': array([[[0.08925092, 0.773956  , 0.6545715 , ..., 0.44341415,\n",
      "         0.45045954, 0.22723871],\n",
      "        [0.09213591, 0.55458474, 0.8878898 , ..., 0.7447621 ,\n",
      "         0.36664265, 0.9675097 ],\n",
      "        [0.41085035, 0.32582533, 0.90553576, ..., 0.38747835,\n",
      "         0.8980876 , 0.28832805],\n",
      "        ...,\n",
      "        [0.41836828, 0.5780172 , 0.5375471 , ..., 0.6346291 ,\n",
      "         0.9714626 , 0.41181087],\n",
      "        [0.15344363, 0.40878308, 0.8401149 , ..., 0.7874322 ,\n",
      "         0.3427019 , 0.5491443 ],\n",
      "        [0.19697303, 0.43141818, 0.5296637 , ..., 0.07205909,\n",
      "         0.8685205 , 0.84199315]]], dtype=float32), 'label': np.int64(1)}}\n",
      "[2025-07-22 13:22:50,081 hyrax.models.hyrax_autoencoder:WARNING] Found shape: torch.Size([2, 32, 32]) in data sample, using this to initialize model.\n",
      "[2025-07-22 13:22:50,085 hyrax.models.model_registry:INFO] Using criterion: torch.nn.CrossEntropyLoss with default arguments.\n",
      "2025-07-22 13:22:50,115 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'rando_0\n",
      "  Dataset cl': \n",
      "\t{'pin_memory': True, 'batch_size': 512, 'shuffle': False}\n",
      "2025-07-22 13:22:50,116 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'rando_0\n",
      "  Dataset cl': \n",
      "\t{'pin_memory': True, 'batch_size': 512, 'shuffle': False}\n",
      "/Users/drew/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/ignite/handlers/tqdm_logger.py:127: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "2025/07/22 13:22:50 INFO mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics. Set logger level to DEBUG for more details.\n",
      "2025/07/22 13:22:50 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "[2025-07-22 13:22:50,229 hyrax.pytorch_ignite:INFO] Training model on device: mps\n",
      "Current run is terminating due to exception: 'HyraxRandomIterableDataset' object has no attribute 'get_object_id'\n",
      "Engine run is terminating due to exception: 'HyraxRandomIterableDataset' object has no attribute 'get_object_id'\n",
      "2025/07/22 13:22:50 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2025/07/22 13:22:50 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HyraxRandomIterableDataset' object has no attribute 'get_object_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/verbs/train.py:90\u001b[39m, in \u001b[36mTrain.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     87\u001b[39m     Train._log_params(config, results_dir)\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# Run the training process\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepochs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[32m     93\u001b[39m model.save(results_dir / config[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mweights_filepath\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/ignite/engine/engine.py:889\u001b[39m, in \u001b[36mEngine.run\u001b[39m\u001b[34m(self, data, max_epochs, epoch_length)\u001b[39m\n\u001b[32m    886\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.dataloader = data\n\u001b[32m    888\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.interrupt_resume_enabled:\n\u001b[32m--> \u001b[39m\u001b[32m889\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_internal_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    891\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._internal_run_legacy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/ignite/engine/engine.py:932\u001b[39m, in \u001b[36mEngine._internal_run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    930\u001b[39m     \u001b[38;5;28mself\u001b[39m._internal_run_generator = \u001b[38;5;28mself\u001b[39m._internal_run_as_gen()\n\u001b[32m    931\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m932\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_internal_run_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[32m    934\u001b[39m     \u001b[38;5;28mself\u001b[39m._internal_run_generator = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/ignite/engine/engine.py:990\u001b[39m, in \u001b[36mEngine._internal_run_as_gen\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataloader_iter = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    989\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEngine run is terminating due to exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m990\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[38;5;28mself\u001b[39m._dataloader_iter = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/ignite/engine/engine.py:644\u001b[39m, in \u001b[36mEngine._handle_exception\u001b[39m\u001b[34m(self, e)\u001b[39m\n\u001b[32m    642\u001b[39m     \u001b[38;5;28mself\u001b[39m._fire_event(Events.EXCEPTION_RAISED, e)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/ignite/engine/engine.py:956\u001b[39m, in \u001b[36mEngine._internal_run_as_gen\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataloader_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    954\u001b[39m     \u001b[38;5;28mself\u001b[39m._setup_engine()\n\u001b[32m--> \u001b[39m\u001b[32m956\u001b[39m epoch_time_taken += \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_once_on_dataset_as_gen()\n\u001b[32m    958\u001b[39m \u001b[38;5;66;03m# time is available for handlers but must be updated after fire\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[38;5;28mself\u001b[39m.state.times[Events.EPOCH_COMPLETED.name] = epoch_time_taken\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/ignite/engine/engine.py:1096\u001b[39m, in \u001b[36mEngine._run_once_on_dataset_as_gen\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrent run is terminating due to exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1096\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/ignite/engine/engine.py:644\u001b[39m, in \u001b[36mEngine._handle_exception\u001b[39m\u001b[34m(self, e)\u001b[39m\n\u001b[32m    642\u001b[39m     \u001b[38;5;28mself\u001b[39m._fire_event(Events.EXCEPTION_RAISED, e)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/ignite/engine/engine.py:1033\u001b[39m, in \u001b[36mEngine._run_once_on_dataset_as_gen\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1030\u001b[39m         \u001b[38;5;28mself\u001b[39m._fire_event(Events.GET_BATCH_STARTED)\n\u001b[32m   1031\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._maybe_terminate_or_interrupt()\n\u001b[32m-> \u001b[39m\u001b[32m1033\u001b[39m \u001b[38;5;28mself\u001b[39m.state.batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[38;5;66;03m# We should not trigger GET_BATCH_STARTED, GET_BATCH_COMPLETED, DATALOADER_STOP_ITERATION events\u001b[39;00m\n\u001b[32m   1036\u001b[39m \u001b[38;5;66;03m# if no data was provided to engine.run(data=None, ...)\u001b[39;00m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.dataloader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/data_sets/data_provider.py:145\u001b[39m, in \u001b[36mDataProvider.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper that allows this class to be used as a PyTorch Dataset.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresolve_data_by_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/data_sets/data_provider.py:184\u001b[39m, in \u001b[36mDataProvider.resolve_data_by_index\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dataset_definition.get(\u001b[33m\"\u001b[39m\u001b[33mfields\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m dataset_definition.get(\u001b[33m\"\u001b[39m\u001b[33mfields\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m         resolved_data = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepped_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfriendly_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfield\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m(idx)\n\u001b[32m    185\u001b[39m         returned_data[friendly_name][field] = resolved_data\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# call __getitem__ on the dataset to get all data. Expect that the\u001b[39;00m\n\u001b[32m    188\u001b[39m     \u001b[38;5;66;03m# returned data is a dictionary with a default set of fields\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'HyraxRandomIterableDataset' object has no attribute 'get_object_id'"
     ]
    }
   ],
   "source": [
    "h.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9185bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87db3e",
   "metadata": {},
   "source": [
    "# Things get not-so-great after this\n",
    "From here we really need to consider how we save state.\n",
    "We need to make sure that the datasets collected in ``DataProvider`` are copied to the config.\n",
    "We need to figure out how to recreate the current DataProvider from the config.\n",
    "And we need to figure out when to use the contents of the persisted config file vs.\n",
    "when to use the ``data`` attribute in the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe6d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyrax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
