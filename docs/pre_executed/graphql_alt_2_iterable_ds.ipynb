{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad04228",
   "metadata": {},
   "source": [
    "# Multi-modal data with a GraphQL alternative\n",
    "\n",
    "This notebook will demonstrate working with multi-modal or multi-source data. By default, Hyrax assumes only a single dataset will be used for a given model.\n",
    "\n",
    "As before, the two required configuration parameters are `config['general']['data_dir']` (to define the location of the data) and `config['data_set']['name']` (to define the dataset class that will load specific data from disk).\n",
    "\n",
    "Below we the standard approach of loading the default dataset defined in the Hyrax config using `h.prepare()`. We then print the dataset object to see the basic information about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d759d831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-22 16:03:22,595 hyrax:INFO] Runtime Config read from: /Users/drew/code/hyrax/src/hyrax/hyrax_default_config.toml\n"
     ]
    }
   ],
   "source": [
    "from hyrax import Hyrax\n",
    "\n",
    "h = Hyrax()\n",
    "\n",
    "# Set a few configs for later use\n",
    "h.config[\"train\"][\"epochs\"] = 1\n",
    "h.config[\"data_set.random_dataset\"][\"shape\"] = (1, 32, 32)\n",
    "h.config[\"data_set.random_dataset\"][\"size\"] = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61210469",
   "metadata": {},
   "source": [
    "## Attaching a iterable datasets to a model\n",
    "The following shows how to set the dataset to be the `HyraxRandomIterableDataset`.\n",
    "\n",
    "When experimentation is complete, the following can also be specified directly in a configuration .toml file like so:\n",
    "```toml\n",
    "[model_data]\n",
    "\n",
    "[model_data.rando]\n",
    "dataset_class = \"HyraxRandomIterableDataset\"\n",
    "data_directory = \"./data\"\n",
    "fields = [\"image\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908025fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.config[\"model_data\"] = {\n",
    "    \"rando_0\": {\n",
    "        \"dataset_class\": \"HyraxRandomIterableDataset\",\n",
    "        \"data_directory\": \"./data\",\n",
    "        \"fields\": [\"object_id\", \"image\"],\n",
    "        \"primary_id_field\": \"object_id\",\n",
    "    },\n",
    "    # \"rando_1\": {\n",
    "    #     \"dataset_class\": \"HyraxRandomIterableDataset\",\n",
    "    #     \"data_directory\": \"./data\",\n",
    "    #     \"fields\": [\"object_id\", \"image\"],\n",
    "    # },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce664770",
   "metadata": {},
   "source": [
    "## Examine the new iterable dataset\n",
    "As before, calling ``h.prepare()`` will return an instance of the ``DataProvider`` dataset.\n",
    "The ``DataProvider`` class can be thought of as a container of multiple datasets, as well as a gateway (in GraphQL terminology)\n",
    "that will send requests for specific data to the datasets it contains. Printing the dataset object will show the configuration of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32452f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-22 16:03:26,605 hyrax.data_sets.data_provider:ERROR] Dataset 'HyraxRandomIterableDataset' is an iterable-style dataset. This is not supported in the current implementation of DataProvider.Hyrax only supports 1-N map-style datasets at this time or single iterable-style datasets.\n",
      "[2025-07-22 16:03:26,606 hyrax.data_sets.data_provider:ERROR] No `get_object_id` method for requested field, 'object_id' was found in dataset HyraxRandomIterableDataset.\n",
      "[2025-07-22 16:03:26,606 hyrax.data_sets.data_provider:ERROR] Finished validating request. Problems found: 2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Data request validation failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ds = \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(ds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/hyrax.py:175\u001b[39m, in \u001b[36mHyrax.prepare\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[33;03mSee Hyrax.prepare.run()\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprepare\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/prepare.py:18\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(config):\n\u001b[32m      9\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prepare the dataset for a given model and data loader.\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[33;03m        dict\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     data_set = \u001b[43msetup_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mFinished Prepare\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data_set\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/pytorch_ignite.py:81\u001b[39m, in \u001b[36msetup_dataset\u001b[39m\u001b[34m(config, tensorboardx_logger)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create a dataset object based on the configuration.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     75\u001b[39m \u001b[33;03m    An instance of the dataset class specified in the configuration\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Fetch data loader class specified in config and create an instance of it\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Fetch the model class defined in the config, and use it's ``data`` attribute\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# to initialize the ``DataProvider`` instance.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_setup_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboardx_logger\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/pytorch_ignite.py:54\u001b[39m, in \u001b[36m_setup_dataset\u001b[39m\u001b[34m(config, tensorboardx_logger)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_setup_dataset\u001b[39m(config, tensorboardx_logger):\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     data_provider = \u001b[43mDataProvider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     data_provider.prepare_datasets()\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m friendly_name \u001b[38;5;129;01min\u001b[39;00m data_provider.prepped_datasets:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/data_sets/data_provider.py:46\u001b[39m, in \u001b[36mDataProvider.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mself\u001b[39m.data_request = data_request\n\u001b[32m     44\u001b[39m \u001b[38;5;28mself\u001b[39m.config = config\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m.prepped_datasets = {}\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m.all_metadata_fields = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/data_sets/data_provider.py:147\u001b[39m, in \u001b[36mDataProvider.validate_request\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m problem_count > \u001b[32m0\u001b[39m:\n\u001b[32m    146\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinished validating request. Problems found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblem_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mData request validation failed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Data request validation failed."
     ]
    }
   ],
   "source": [
    "ds = h.prepare()\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c36843",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.get_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Is iterable: {ds.is_iterable()}\")  # Should return True\n",
    "print(f\"Is mappable: {ds.is_map()}\")  # Should return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8959af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = ds.get_sample()\n",
    "print(\"Fields from rando_0\")\n",
    "print(samp[\"rando_0\"][\"image\"].shape)\n",
    "# print(\"Fields from rando_1\")\n",
    "# print(samp[\"rando_1\"][\"image\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3368ea",
   "metadata": {},
   "source": [
    "## Pass the data through ``to_tensor``\n",
    "Since we have access to the model class, we can call the ``to_tensor`` method with example data.\n",
    "This allows easy checking that the output matches the expectations of the model architecture.\n",
    "\n",
    "In this example, we expect ``to_tensor`` to return a tuple of (Tensor, int), or specifically a multi-channel image and a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86eb74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "@staticmethod\n",
    "def to_tensor(data_dict):\n",
    "    \"\"\"This function converts structured data to the input tensor we need to run\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dict : dict\n",
    "        The dictionary returned from our data source\n",
    "    \"\"\"\n",
    "    rando_0 = data_dict.get(\"rando_0\", {})\n",
    "    rando_1 = data_dict.get(\"rando_1\", {})\n",
    "\n",
    "    if \"image\" in rando_0:\n",
    "        image_0 = torch.from_numpy(rando_0[\"image\"])\n",
    "\n",
    "    if \"image\" in rando_1:\n",
    "        image_1 = torch.from_numpy(rando_1[\"image\"])\n",
    "\n",
    "        stack_dim = 0 if image_0.ndim == 3 else 1\n",
    "        image_0 = torch.from_numpy(np.concatenate([image_0, image_1], axis=stack_dim))\n",
    "\n",
    "    return image_0\n",
    "\n",
    "\n",
    "m = h.model()\n",
    "m.to_tensor = to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e539239",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = m.to_tensor(samp)\n",
    "print(f\"Type and shape of resulting image: {type(res)}, {res.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c917f4",
   "metadata": {},
   "source": [
    "## Train with this model\n",
    "Now that we've seen that the ``to_tensor`` method is returning a reasonable form of data, we can train our model.\n",
    "As before, we call ``h.train()``.\n",
    "While it is quiet verbose, the initialization logging shows that the model instance is created with data from\n",
    "the ``DataProvider`` class, and that our new implementation of ``to_tensor`` is being used to manipulate\n",
    "the data from ``DataProvider`` into the a form that our model architecture accepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d1861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9185bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87db3e",
   "metadata": {},
   "source": [
    "# Things get not-so-great after this\n",
    "From here we really need to consider how we save state.\n",
    "We need to make sure that the datasets collected in ``DataProvider`` are copied to the config.\n",
    "We need to figure out how to recreate the current DataProvider from the config.\n",
    "And we need to figure out when to use the contents of the persisted config file vs.\n",
    "when to use the ``data`` attribute in the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe6d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af6269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, IterableDataset\n",
    "from hyrax.data_sets.data_set_registry import HyraxDataset\n",
    "\n",
    "\n",
    "class Thing(HyraxDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return 100\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"image\": torch.randn(3, 32, 32), \"label\": idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Thing.is_iterable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042a2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyrax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
