{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad04228",
   "metadata": {},
   "source": [
    "# Multi-modal data with a GraphQL alternative\n",
    "\n",
    "This notebook will demonstrate working with multi-modal or multi-source data. By default, Hyrax assumes only a single dataset will be used for a given model.\n",
    "\n",
    "As before, the two required configuration parameters are `config['general']['data_dir']` (to define the location of the data) and `config['data_set']['name']` (to define the dataset class that will load specific data from disk).\n",
    "\n",
    "Below we the standard approach of loading the default dataset defined in the Hyrax config using `h.prepare()`. We then print the dataset object to see the basic information about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d759d831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-21 13:47:16,052 hyrax:INFO] Runtime Config read from: /Users/drew/code/hyrax/src/hyrax/hyrax_default_config.toml\n"
     ]
    }
   ],
   "source": [
    "from hyrax import Hyrax\n",
    "\n",
    "h = Hyrax()\n",
    "\n",
    "# Set a few configs for later use\n",
    "h.config[\"train\"][\"epochs\"] = 1\n",
    "h.config[\"data_set.random_dataset\"][\"shape\"] = (1, 32, 32)\n",
    "h.config[\"data_set.random_dataset\"][\"size\"] = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61210469",
   "metadata": {},
   "source": [
    "## Attaching a iterable datasets to a model\n",
    "The following shows how to set the dataset to be the `HyraxRandomIterableDataset`.\n",
    "\n",
    "When experimentation is complete, the following can also be specified directly in a configuration .toml file like so:\n",
    "```toml\n",
    "[model_data]\n",
    "\n",
    "[model_data.rando]\n",
    "dataset_class = \"HyraxRandomIterableDataset\"\n",
    "data_directory = \"./data\"\n",
    "fields = [\"image\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908025fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.config[\"model_data\"] = {\n",
    "    \"rando\": {\n",
    "        \"dataset_class\": \"HyraxRandomIterableDataset\",\n",
    "        \"data_directory\": \"./data\",\n",
    "        \"fields\": [\"object_id\", \"image\"],\n",
    "        \"primary_id_field\": \"object_id\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce664770",
   "metadata": {},
   "source": [
    "## Examine the new iterable dataset\n",
    "As before, calling ``h.prepare()`` will return an instance of the ``DataProvider`` dataset.\n",
    "The ``DataProvider`` class can be thought of as a container of multiple datasets, as well as a gateway (in GraphQL terminology)\n",
    "that will send requests for specific data to the datasets it contains. Printing the dataset object will show the configuration of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32452f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-21 13:47:22,319 hyrax.prepare:INFO] Finished Prepare\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rando\n",
      "  Dataset class: HyraxRandomIterableDataset\n",
      "  Data directory: ./data\n",
      "  Fields: object_id, image\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = h.prepare()\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b50c7",
   "metadata": {},
   "source": [
    "The various datasets contained within the `DataProvider` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b10a6734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rando': <hyrax.data_sets.random.hyrax_random_dataset.HyraxRandomIterableDataset at 0x17abf5310>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.prepped_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5939262a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is iterable: True\n",
      "Is mappable: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is iterable: {ds.is_iterable()}\")  # Should return True\n",
    "print(f\"Is mappable: {ds.is_map()}\")  # Should return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d289f",
   "metadata": {},
   "source": [
    "Checking the length of the dataset is the same as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0256b201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-21 13:47:25,887 hyrax.data_sets.data_provider:ERROR] Primary dataset is iterable, cannot determine length.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLength of the multimodal dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the multimodal dataset: {len(ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8959af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields from cifar_0\n",
      "(3, 32, 32)\n",
      "5\n",
      "2335\n",
      "Fields from cifar_1\n",
      "(3, 32, 32)\n",
      "5\n",
      "Fields from rando\n",
      "(1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "samp = ds[2335]\n",
    "print(\"Fields from cifar_0\")\n",
    "print(samp[\"cifar_0\"][\"image\"].shape)\n",
    "print(samp[\"cifar_0\"][\"label\"])\n",
    "print(samp[\"cifar_0\"][\"object_id\"])\n",
    "print(\"Fields from cifar_1\")\n",
    "print(samp[\"cifar_1\"][\"image\"].shape)\n",
    "print(samp[\"cifar_1\"][\"label\"])\n",
    "print(\"Fields from rando\")\n",
    "print(samp[\"rando\"][\"image\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3368ea",
   "metadata": {},
   "source": [
    "## Pass the data through ``to_tensor``\n",
    "Since we have access to the model class, we can call the ``to_tensor`` method with example data.\n",
    "This allows easy checking that the output matches the expectations of the model architecture.\n",
    "\n",
    "In this example, we expect ``to_tensor`` to return a tuple of (Tensor, int), or specifically a multi-channel image and a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e539239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type and shape of resulting image: <class 'torch.Tensor'>, torch.Size([4, 32, 32])\n",
      "Type and shape of the label: <class 'int'>, 5\n"
     ]
    }
   ],
   "source": [
    "m = h.model()\n",
    "res = m.to_tensor(samp)\n",
    "print(f\"Type and shape of resulting image: {type(res[0])}, {res[0].shape}\")\n",
    "print(f\"Type and shape of the label: {type(res[1])}, {res[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c072c62",
   "metadata": {},
   "source": [
    "## Updating ``to_tensor``\n",
    "The default implementation of ``to_tensor`` only makes use of \"cifar_0\" and \"rando\".\n",
    "But if we are experimenting, we don't want to have to make code changes in the model class.\n",
    "It would be much easier to experiment with in the notebook.\n",
    "Here, we redefine the ``to_tensor`` method, and check the results by running sample data through the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaf466bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@staticmethod\n",
    "def to_tensor(data_dict):\n",
    "    \"\"\"This function converts structured data to the input tensor we need to run\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dict : dict\n",
    "        The dictionary returned from our data source\n",
    "    \"\"\"\n",
    "    cifar_data = data_dict.get(\"cifar_0\", {})\n",
    "    random_data = data_dict.get(\"rando\", {})\n",
    "\n",
    "    more_cifar_data = data_dict.get(\"cifar_1\", {})\n",
    "\n",
    "    if \"label\" in cifar_data:\n",
    "        label = cifar_data[\"label\"]\n",
    "\n",
    "    if \"image\" in cifar_data and \"image\" in random_data:\n",
    "        cifar_image = cifar_data[\"image\"]\n",
    "        random_image = random_data[\"image\"]\n",
    "        more_cifar_image = more_cifar_data[\"image\"]\n",
    "        stack_dim = 0 if cifar_image.ndim == 3 else 1\n",
    "        image = torch.from_numpy(\n",
    "            np.concatenate([cifar_image, random_image, more_cifar_image], axis=stack_dim)\n",
    "        )\n",
    "    elif \"image\" in cifar_data:\n",
    "        image = cifar_data[\"image\"]\n",
    "    elif \"image\" in random_data:\n",
    "        image = torch.from_numpy(random_data[\"image\"])\n",
    "\n",
    "    return (image, label)\n",
    "\n",
    "\n",
    "m.to_tensor = to_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb2a2c",
   "metadata": {},
   "source": [
    "After running the same sample through as before, we can see that the number of channels\n",
    "in the image has changed (from 4 to 7), while all the other values have remained the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b748605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type and shape of resulting image: <class 'torch.Tensor'>, torch.Size([7, 32, 32])\n",
      "Type and shape of the label: <class 'int'>, 5\n"
     ]
    }
   ],
   "source": [
    "new_res = m.to_tensor(samp)\n",
    "print(f\"Type and shape of resulting image: {type(new_res[0])}, {new_res[0].shape}\")\n",
    "print(f\"Type and shape of the label: {type(new_res[1])}, {new_res[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c917f4",
   "metadata": {},
   "source": [
    "## Train with this model\n",
    "Now that we've seen that the ``to_tensor`` method is returning a reasonable form of data, we can train our model.\n",
    "As before, we call ``h.train()``.\n",
    "While it is quiet verbose, the initialization logging shows that the model instance is created with data from\n",
    "the ``DataProvider`` class, and that our new implementation of ``to_tensor`` is being used to manipulate\n",
    "the data from ``DataProvider`` into the a form that our model architecture accepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d1861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9185bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87db3e",
   "metadata": {},
   "source": [
    "# Things get not-so-great after this\n",
    "From here we really need to consider how we save state.\n",
    "We need to make sure that the datasets collected in ``DataProvider`` are copied to the config.\n",
    "We need to figure out how to recreate the current DataProvider from the config.\n",
    "And we need to figure out when to use the contents of the persisted config file vs.\n",
    "when to use the ``data`` attribute in the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe6d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyrax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
