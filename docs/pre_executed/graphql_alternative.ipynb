{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad04228",
   "metadata": {},
   "source": [
    "# Multi-modal data with a GraphQL alternative\n",
    "\n",
    "This notebook will demonstrate working with multi-modal or multi-source data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d759d831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-11 17:23:19,759 hyrax:INFO] Runtime Config read from: /Users/drew/code/hyrax/src/hyrax/hyrax_default_config.toml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cifar_1': {'dataset_class': 'HyraxCifarDataSet',\n",
       "  'data_directory': 'path/to/dataset',\n",
       "  'fields': ['image', 'label', 'object_id'],\n",
       "  'primary_id_field': 'object_id'},\n",
       " 'rando': {'dataset_class': 'HyraxRandomDataset',\n",
       "  'data_directory': '/fake/dir',\n",
       "  'fields': ['image']}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hyrax import Hyrax\n",
    "\n",
    "h = Hyrax()\n",
    "\n",
    "# Set a few configs for later use\n",
    "h.config[\"train\"][\"epochs\"] = 1\n",
    "h.config[\"data_set.random_dataset\"][\"shape\"] = (1, 32, 32)\n",
    "h.config[\"data_set.random_dataset\"][\"size\"] = 50000\n",
    "\n",
    "\n",
    "# Return a reference to the model class based on the configuration used to create `h`.\n",
    "# This should feel similar to `ds = h.prepare()` - Perhaps we should rename to `h.data()`???`\n",
    "m = h.model()\n",
    "\n",
    "# Since `data` is a model class attribute, we can print the data dictionary like so\n",
    "m.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61210469",
   "metadata": {},
   "source": [
    "## Attaching datasets to a model\n",
    "The following shows the process of attaching new datasets to the model class.\n",
    "\n",
    "Note - Attempting to add a dataset with a friendly name that already exists will log and error. To _update_ a dataset already attached to a model, first run ``detach_dataset(...)``, then ``attach_dataset(...)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b0c223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-11 17:23:28,452 hyrax.models.model_registry:ERROR] The friendly name 'cifar_1' already exists. If updating, first run `detach_dataset(cifar_1)`, then run `attach_dataset(('cifar_1', Ellipsis))` again.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cifar_1': {'dataset_class': 'HyraxCifarDataSet',\n",
       "  'data_directory': '/Users/drew/code/hyrax/docs/pre_executed/data',\n",
       "  'fields': ['image', 'label', 'object_id']},\n",
       " 'rando': {'dataset_class': 'HyraxRandomDataset',\n",
       "  'data_directory': '/fake/dir',\n",
       "  'fields': ['image']},\n",
       " 'cifar_0': {'dataset_class': 'HyraxCifarDataSet',\n",
       "  'data_directory': '/Users/drew/code/hyrax/docs/pre_executed/data',\n",
       "  'fields': ['image', 'label', 'object_id'],\n",
       "  'primary_id_field': 'object_id'},\n",
       " 'cifar_2': {'dataset_class': 'HyraxCifarDataSet',\n",
       "  'data_directory': 'path/to/cifar/dataset',\n",
       "  'fields': ['image', 'label', 'object_id']},\n",
       " 'random_dataset': {'dataset_class': 'HyraxRandomDataset',\n",
       "  'data_directory': 'path/to/random/dataset',\n",
       "  'fields': ['image']}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This dataset is already defined on the model's default `data` attribute, so this log an error\n",
    "m.attach_dataset(\n",
    "    friendly_name=\"cifar_0\",\n",
    "    dataset_class=\"HyraxCifarDataSet\",\n",
    "    data_directory=h.config[\"general\"][\"data_dir\"],\n",
    "    fields=[\"image\", \"label\", \"object_id\"],\n",
    "    # if `primary_id_field` not specified, none of the fields will be used as the primary id field\n",
    "    primary_id_field=\"object_id\",\n",
    ")\n",
    "\n",
    "# Note that `primary_id_field` is not defined here\n",
    "m.attach_dataset(\n",
    "    friendly_name=\"cifar_1\",\n",
    "    dataset_class=\"HyraxCifarDataSet\",\n",
    "    data_directory=h.config[\"general\"][\"data_dir\"],\n",
    "    fields=[\"image\", \"label\", \"object_id\"],\n",
    ")\n",
    "\n",
    "m.attach_dataset(\n",
    "    friendly_name=\"cifar_2\",\n",
    "    dataset_class=\"HyraxCifarDataSet\",\n",
    "    data_directory=\"path/to/cifar/dataset\",\n",
    "    fields=[\"image\", \"label\", \"object_id\"],\n",
    ")\n",
    "\n",
    "m.attach_dataset(\n",
    "    friendly_name=\"random_dataset\",\n",
    "    dataset_class=\"HyraxRandomDataset\",\n",
    "    data_directory=\"path/to/random/dataset\",\n",
    "    fields=[\"image\"],\n",
    ")\n",
    "\n",
    "m.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4571fead",
   "metadata": {},
   "source": [
    "## Removing dataset from a model\n",
    "The following shows the process of removing a dataset from a model.\n",
    "\n",
    "Note - Attempting to remove a dataset that doesn't exist will log an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc39c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-11 17:23:30,298 hyrax.models.model_registry:ERROR] Cannot remove 'poopfish' from data. These can be removed: ['cifar_1', 'rando', 'cifar_0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cifar_1': {'dataset_class': 'HyraxCifarDataSet',\n",
       "  'data_directory': '/Users/drew/code/hyrax/docs/pre_executed/data',\n",
       "  'fields': ['image', 'label', 'object_id']},\n",
       " 'rando': {'dataset_class': 'HyraxRandomDataset',\n",
       "  'data_directory': '/fake/dir',\n",
       "  'fields': ['image']},\n",
       " 'cifar_0': {'dataset_class': 'HyraxCifarDataSet',\n",
       "  'data_directory': '/Users/drew/code/hyrax/docs/pre_executed/data',\n",
       "  'fields': ['image', 'label', 'object_id'],\n",
       "  'primary_id_field': 'object_id'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the specific dataset from `data`.\n",
    "m.detach_dataset(friendly_name=\"cifar_2\")\n",
    "m.detach_dataset(friendly_name=\"random_dataset\")\n",
    "\n",
    "# This dataset doesn't exist and will log an error.\n",
    "m.detach_dataset(\"poopfish\")\n",
    "\n",
    "m.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce664770",
   "metadata": {},
   "source": [
    "## Examine the multimodal dataset\n",
    "As before, calling ``h.prepare()`` will return an instance of the ``DataProvider`` dataset.\n",
    "The ``DataProvider`` class can be thought of as a container of multiple datasets, as well as a gateway (in GraphQL terminology)\n",
    "that will send requests for specific data to the datasets it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b4bf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-11 17:23:42,202 hyrax.prepare:INFO] Finished Prepare\n"
     ]
    }
   ],
   "source": [
    "ds = h.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b50c7",
   "metadata": {},
   "source": [
    "The various datasets contained within the `DataProvider` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b10a6734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cifar_1': <hyrax.data_sets.hyrax_cifar_data_set.HyraxCifarDataSet at 0x158343a40>,\n",
       " 'rando': <hyrax.data_sets.random.hyrax_random_dataset.HyraxRandomDataset at 0x14fadcc80>,\n",
       " 'cifar_0': <hyrax.data_sets.hyrax_cifar_data_set.HyraxCifarDataSet at 0x158341580>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.prepped_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d289f",
   "metadata": {},
   "source": [
    "Checking the length of the dataset is the same as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0256b201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the multimodal dataset: 50000\n",
      "Length of a specific dataset contained inside: 50000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of the multimodal dataset: {len(ds)}\")\n",
    "print(f\"Length of a specific dataset contained inside: {len(ds.prepped_datasets['cifar_0'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8959af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields from cifar_0\n",
      "(3, 32, 32)\n",
      "5\n",
      "2335\n",
      "Fields from cifar_1\n",
      "(3, 32, 32)\n",
      "5\n",
      "2335\n",
      "Fields from rando\n",
      "(1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "samp = ds[2335]\n",
    "print(\"Fields from cifar_0\")\n",
    "print(samp[\"cifar_0\"][\"image\"].shape)\n",
    "print(samp[\"cifar_0\"][\"label\"])\n",
    "print(samp[\"cifar_0\"][\"object_id\"])\n",
    "print(\"Fields from cifar_1\")\n",
    "print(samp[\"cifar_1\"][\"image\"].shape)\n",
    "print(samp[\"cifar_1\"][\"label\"])\n",
    "print(samp[\"cifar_1\"][\"object_id\"])\n",
    "print(\"Fields from rando\")\n",
    "print(samp[\"rando\"][\"image\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3368ea",
   "metadata": {},
   "source": [
    "## Pass the data through ``to_tensor``\n",
    "Since we have access to the model class, we can call the ``to_tensor`` method with example data.\n",
    "This allows easy checking that the output matches the expectations of the model architecture.\n",
    "\n",
    "In this example, we expect ``to_tensor`` to return a tuple of (Tensor, int), or specifically a multi-channel image and a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e539239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type and shape of resulting image: <class 'torch.Tensor'>, torch.Size([4, 32, 32])\n",
      "Type and shape of the label: <class 'int'>, 5\n"
     ]
    }
   ],
   "source": [
    "res = m.to_tensor(samp)\n",
    "print(f\"Type and shape of resulting image: {type(res[0])}, {res[0].shape}\")\n",
    "print(f\"Type and shape of the label: {type(res[1])}, {res[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c072c62",
   "metadata": {},
   "source": [
    "## Updating ``to_tensor``\n",
    "The default implementation of ``to_tensor`` only makes use of \"cifar_0\" and \"rando\".\n",
    "But if we are experimenting, we don't want to have to make code changes in the model class.\n",
    "It would be much easier to experiment with in the notebook.\n",
    "Here, we redefine the ``to_tensor`` method, and check the results by running sample data through the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaf466bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@staticmethod\n",
    "def to_tensor(data_dict):\n",
    "    \"\"\"This function converts structured data to the input tensor we need to run\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dict : dict\n",
    "        The dictionary returned from our data source\n",
    "    \"\"\"\n",
    "    cifar_data = data_dict.get(\"cifar_0\", {})\n",
    "    random_data = data_dict.get(\"rando\", {})\n",
    "\n",
    "    more_cifar_data = data_dict.get(\"cifar_1\", {})\n",
    "\n",
    "    if \"label\" in cifar_data:\n",
    "        label = cifar_data[\"label\"]\n",
    "\n",
    "    if \"image\" in cifar_data and \"image\" in random_data:\n",
    "        cifar_image = cifar_data[\"image\"]\n",
    "        random_image = random_data[\"image\"]\n",
    "        more_cifar_image = more_cifar_data[\"image\"]\n",
    "        stack_dim = 0 if cifar_image.ndim == 3 else 1\n",
    "        image = torch.from_numpy(\n",
    "            np.concatenate([cifar_image, random_image, more_cifar_image], axis=stack_dim)\n",
    "        )\n",
    "    elif \"image\" in cifar_data:\n",
    "        image = cifar_data[\"image\"]\n",
    "    elif \"image\" in random_data:\n",
    "        image = torch.from_numpy(random_data[\"image\"])\n",
    "\n",
    "    return (image, label)\n",
    "\n",
    "\n",
    "m.to_tensor = to_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb2a2c",
   "metadata": {},
   "source": [
    "After running the same sample through as before, we can see that the number of channels\n",
    "in the image has changed (from 4 to 7), while all the other values have remained the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b748605c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type and shape of resulting image: <class 'torch.Tensor'>, torch.Size([7, 32, 32])\n",
      "Type and shape of the label: <class 'int'>, 5\n"
     ]
    }
   ],
   "source": [
    "new_res = m.to_tensor(samp)\n",
    "print(f\"Type and shape of resulting image: {type(new_res[0])}, {new_res[0].shape}\")\n",
    "print(f\"Type and shape of the label: {type(new_res[1])}, {new_res[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c917f4",
   "metadata": {},
   "source": [
    "## Train with this model\n",
    "Now that we've seen that the ``to_tensor`` method is returning a reasonable form of data, we can train our model.\n",
    "As before, we call ``h.train()``.\n",
    "While it is quiet verbose, the initialization logging shows that the model instance is created with data from\n",
    "the ``DataProvider`` class, and that our new implementation of ``to_tensor`` is being used to manipulate\n",
    "the data from ``DataProvider`` into the a form that our model architecture accepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33d1861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-11 17:24:43,776 hyrax.models.hyrax_autoencoder:WARNING] Initializing HyraxAutoencoder with data sample: {'cifar_1': {'image': array([[[-0.5372549 , -0.6627451 , -0.60784316, ...,  0.23921573,\n",
      "          0.19215691,  0.16078436],\n",
      "        [-0.8745098 , -1.        , -0.85882354, ..., -0.03529412,\n",
      "         -0.06666666, -0.04313725],\n",
      "        [-0.8039216 , -0.8745098 , -0.6156863 , ..., -0.0745098 ,\n",
      "         -0.05882353, -0.14509803],\n",
      "        ...,\n",
      "        [ 0.6313726 ,  0.5764706 ,  0.5529412 , ...,  0.254902  ,\n",
      "         -0.56078434, -0.58431375],\n",
      "        [ 0.41176474,  0.35686278,  0.45882356, ...,  0.4431373 ,\n",
      "         -0.23921567, -0.3490196 ],\n",
      "        [ 0.38823533,  0.3176471 ,  0.4039216 , ...,  0.69411767,\n",
      "          0.18431377, -0.03529412]],\n",
      "\n",
      "       [[-0.5137255 , -0.6392157 , -0.62352943, ...,  0.03529418,\n",
      "         -0.01960784, -0.02745098],\n",
      "        [-0.84313726, -1.        , -0.9372549 , ..., -0.3098039 ,\n",
      "         -0.3490196 , -0.31764704],\n",
      "        [-0.8117647 , -0.94509804, -0.7882353 , ..., -0.34117645,\n",
      "         -0.34117645, -0.42745095],\n",
      "        ...,\n",
      "        [ 0.33333337,  0.20000005,  0.26274514, ...,  0.04313731,\n",
      "         -0.75686276, -0.73333335],\n",
      "        [ 0.09019613, -0.03529412,  0.12941182, ...,  0.16078436,\n",
      "         -0.5137255 , -0.58431375],\n",
      "        [ 0.12941182,  0.01176476,  0.11372554, ...,  0.4431373 ,\n",
      "         -0.0745098 , -0.27843136]],\n",
      "\n",
      "       [[-0.5058824 , -0.64705884, -0.6627451 , ..., -0.15294117,\n",
      "         -0.19999999, -0.19215685],\n",
      "        [-0.84313726, -1.        , -1.        , ..., -0.5686275 ,\n",
      "         -0.60784316, -0.5529412 ],\n",
      "        [-0.8352941 , -1.        , -0.9372549 , ..., -0.60784316,\n",
      "         -0.60784316, -0.67058825],\n",
      "        ...,\n",
      "        [-0.24705881, -0.73333335, -0.79607844, ..., -0.45098037,\n",
      "         -0.94509804, -0.84313726],\n",
      "        [-0.24705881, -0.67058825, -0.7647059 , ..., -0.26274508,\n",
      "         -0.73333335, -0.73333335],\n",
      "        [-0.09019607, -0.26274508, -0.31764704, ...,  0.09803927,\n",
      "         -0.34117645, -0.4352941 ]]], dtype=float32), 'label': 6, 'object_id': 0}, 'rando': {'image': array([[[0.08925092, 0.773956  , 0.6545715 , ..., 0.44341415,\n",
      "         0.45045954, 0.22723871],\n",
      "        [0.09213591, 0.55458474, 0.8878898 , ..., 0.7447621 ,\n",
      "         0.36664265, 0.9675097 ],\n",
      "        [0.41085035, 0.32582533, 0.90553576, ..., 0.38747835,\n",
      "         0.8980876 , 0.28832805],\n",
      "        ...,\n",
      "        [0.41836828, 0.5780172 , 0.5375471 , ..., 0.6346291 ,\n",
      "         0.9714626 , 0.41181087],\n",
      "        [0.15344363, 0.40878308, 0.8401149 , ..., 0.7874322 ,\n",
      "         0.3427019 , 0.5491443 ],\n",
      "        [0.19697303, 0.43141818, 0.5296637 , ..., 0.07205909,\n",
      "         0.8685205 , 0.84199315]]], dtype=float32)}, 'cifar_0': {'image': array([[[-0.5372549 , -0.6627451 , -0.60784316, ...,  0.23921573,\n",
      "          0.19215691,  0.16078436],\n",
      "        [-0.8745098 , -1.        , -0.85882354, ..., -0.03529412,\n",
      "         -0.06666666, -0.04313725],\n",
      "        [-0.8039216 , -0.8745098 , -0.6156863 , ..., -0.0745098 ,\n",
      "         -0.05882353, -0.14509803],\n",
      "        ...,\n",
      "        [ 0.6313726 ,  0.5764706 ,  0.5529412 , ...,  0.254902  ,\n",
      "         -0.56078434, -0.58431375],\n",
      "        [ 0.41176474,  0.35686278,  0.45882356, ...,  0.4431373 ,\n",
      "         -0.23921567, -0.3490196 ],\n",
      "        [ 0.38823533,  0.3176471 ,  0.4039216 , ...,  0.69411767,\n",
      "          0.18431377, -0.03529412]],\n",
      "\n",
      "       [[-0.5137255 , -0.6392157 , -0.62352943, ...,  0.03529418,\n",
      "         -0.01960784, -0.02745098],\n",
      "        [-0.84313726, -1.        , -0.9372549 , ..., -0.3098039 ,\n",
      "         -0.3490196 , -0.31764704],\n",
      "        [-0.8117647 , -0.94509804, -0.7882353 , ..., -0.34117645,\n",
      "         -0.34117645, -0.42745095],\n",
      "        ...,\n",
      "        [ 0.33333337,  0.20000005,  0.26274514, ...,  0.04313731,\n",
      "         -0.75686276, -0.73333335],\n",
      "        [ 0.09019613, -0.03529412,  0.12941182, ...,  0.16078436,\n",
      "         -0.5137255 , -0.58431375],\n",
      "        [ 0.12941182,  0.01176476,  0.11372554, ...,  0.4431373 ,\n",
      "         -0.0745098 , -0.27843136]],\n",
      "\n",
      "       [[-0.5058824 , -0.64705884, -0.6627451 , ..., -0.15294117,\n",
      "         -0.19999999, -0.19215685],\n",
      "        [-0.84313726, -1.        , -1.        , ..., -0.5686275 ,\n",
      "         -0.60784316, -0.5529412 ],\n",
      "        [-0.8352941 , -1.        , -0.9372549 , ..., -0.60784316,\n",
      "         -0.60784316, -0.67058825],\n",
      "        ...,\n",
      "        [-0.24705881, -0.73333335, -0.79607844, ..., -0.45098037,\n",
      "         -0.94509804, -0.84313726],\n",
      "        [-0.24705881, -0.67058825, -0.7647059 , ..., -0.26274508,\n",
      "         -0.73333335, -0.73333335],\n",
      "        [-0.09019607, -0.26274508, -0.31764704, ...,  0.09803927,\n",
      "         -0.34117645, -0.4352941 ]]], dtype=float32), 'label': 6, 'object_id': 0}, 'object_id': 0}\n",
      "[2025-07-11 17:24:43,777 hyrax.models.hyrax_autoencoder:WARNING] Found shape: torch.Size([7, 32, 32]) in data sample, using this to initialize model.\n",
      "[2025-07-11 17:24:43,781 hyrax.models.model_registry:INFO] Using criterion: torch.nn.CrossEntropyLoss with default arguments.\n",
      "2025-07-11 17:24:43,797 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<hyrax.data_sets.dat': \n",
      "\t{'sampler': <hyrax.pytorch_ignite.SubsetSequentialSampler object at 0x158599400>, 'batch_size': 512, 'shuffle': False, 'pin_memory': False}\n",
      "2025-07-11 17:24:43,797 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<hyrax.data_sets.dat': \n",
      "\t{'sampler': <hyrax.pytorch_ignite.SubsetSequentialSampler object at 0x1585997c0>, 'batch_size': 512, 'shuffle': False, 'pin_memory': False}\n",
      "/Users/drew/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/ignite/handlers/tqdm_logger.py:127: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "2025/07/11 17:24:43 INFO mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics. Set logger level to DEBUG for more details.\n",
      "2025/07/11 17:24:43 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "[2025-07-11 17:24:43,926 hyrax.pytorch_ignite:INFO] Training model on device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6789cf03e5447bbbd06997f09c186e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  2%|1         | 1/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-11 17:24:59,693 hyrax.pytorch_ignite:INFO] Total training time: 15.77[s]\n",
      "[2025-07-11 17:24:59,693 hyrax.pytorch_ignite:INFO] Latest checkpoint saved as: /Users/drew/code/hyrax/docs/pre_executed/results/20250711-172436-train-YSo5/checkpoint_epoch_1.pt\n",
      "[2025-07-11 17:24:59,694 hyrax.pytorch_ignite:INFO] Best metric checkpoint saved as: /Users/drew/code/hyrax/docs/pre_executed/results/20250711-172436-train-YSo5/checkpoint_1_loss=-951.7826.pt\n",
      "2025/07/11 17:24:59 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2025/07/11 17:24:59 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n",
      "[2025-07-11 17:24:59,709 hyrax.verbs.train:INFO] Finished Training\n",
      "[2025-07-11 17:25:00,374 hyrax.model_exporters:INFO] Exported model to ONNX format: /Users/drew/code/hyrax/docs/pre_executed/results/20250711-172436-train-YSo5/example_model_opset_20.onnx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HyraxAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(7, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): GELU(approximate='none')\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (5): GELU(approximate='none')\n",
       "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): GELU(approximate='none')\n",
       "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (9): GELU(approximate='none')\n",
       "    (10): Flatten(start_dim=1, end_dim=-1)\n",
       "    (11): Linear(in_features=1024, out_features=64, bias=True)\n",
       "  )\n",
       "  (dec_linear): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=1024, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): GELU(approximate='none')\n",
       "    (4): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (5): GELU(approximate='none')\n",
       "    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): GELU(approximate='none')\n",
       "    (8): ConvTranspose2d(32, 7, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (9): Tanh()\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9185bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-11 17:25:30,971 hyrax.models.hyrax_autoencoder:WARNING] Initializing HyraxAutoencoder with data sample: {'cifar_1': {'image': array([[[-0.5372549 , -0.6627451 , -0.60784316, ...,  0.23921573,\n",
      "          0.19215691,  0.16078436],\n",
      "        [-0.8745098 , -1.        , -0.85882354, ..., -0.03529412,\n",
      "         -0.06666666, -0.04313725],\n",
      "        [-0.8039216 , -0.8745098 , -0.6156863 , ..., -0.0745098 ,\n",
      "         -0.05882353, -0.14509803],\n",
      "        ...,\n",
      "        [ 0.6313726 ,  0.5764706 ,  0.5529412 , ...,  0.254902  ,\n",
      "         -0.56078434, -0.58431375],\n",
      "        [ 0.41176474,  0.35686278,  0.45882356, ...,  0.4431373 ,\n",
      "         -0.23921567, -0.3490196 ],\n",
      "        [ 0.38823533,  0.3176471 ,  0.4039216 , ...,  0.69411767,\n",
      "          0.18431377, -0.03529412]],\n",
      "\n",
      "       [[-0.5137255 , -0.6392157 , -0.62352943, ...,  0.03529418,\n",
      "         -0.01960784, -0.02745098],\n",
      "        [-0.84313726, -1.        , -0.9372549 , ..., -0.3098039 ,\n",
      "         -0.3490196 , -0.31764704],\n",
      "        [-0.8117647 , -0.94509804, -0.7882353 , ..., -0.34117645,\n",
      "         -0.34117645, -0.42745095],\n",
      "        ...,\n",
      "        [ 0.33333337,  0.20000005,  0.26274514, ...,  0.04313731,\n",
      "         -0.75686276, -0.73333335],\n",
      "        [ 0.09019613, -0.03529412,  0.12941182, ...,  0.16078436,\n",
      "         -0.5137255 , -0.58431375],\n",
      "        [ 0.12941182,  0.01176476,  0.11372554, ...,  0.4431373 ,\n",
      "         -0.0745098 , -0.27843136]],\n",
      "\n",
      "       [[-0.5058824 , -0.64705884, -0.6627451 , ..., -0.15294117,\n",
      "         -0.19999999, -0.19215685],\n",
      "        [-0.84313726, -1.        , -1.        , ..., -0.5686275 ,\n",
      "         -0.60784316, -0.5529412 ],\n",
      "        [-0.8352941 , -1.        , -0.9372549 , ..., -0.60784316,\n",
      "         -0.60784316, -0.67058825],\n",
      "        ...,\n",
      "        [-0.24705881, -0.73333335, -0.79607844, ..., -0.45098037,\n",
      "         -0.94509804, -0.84313726],\n",
      "        [-0.24705881, -0.67058825, -0.7647059 , ..., -0.26274508,\n",
      "         -0.73333335, -0.73333335],\n",
      "        [-0.09019607, -0.26274508, -0.31764704, ...,  0.09803927,\n",
      "         -0.34117645, -0.4352941 ]]], dtype=float32), 'label': 6, 'object_id': 0}, 'rando': {'image': array([[[0.08925092, 0.773956  , 0.6545715 , ..., 0.44341415,\n",
      "         0.45045954, 0.22723871],\n",
      "        [0.09213591, 0.55458474, 0.8878898 , ..., 0.7447621 ,\n",
      "         0.36664265, 0.9675097 ],\n",
      "        [0.41085035, 0.32582533, 0.90553576, ..., 0.38747835,\n",
      "         0.8980876 , 0.28832805],\n",
      "        ...,\n",
      "        [0.41836828, 0.5780172 , 0.5375471 , ..., 0.6346291 ,\n",
      "         0.9714626 , 0.41181087],\n",
      "        [0.15344363, 0.40878308, 0.8401149 , ..., 0.7874322 ,\n",
      "         0.3427019 , 0.5491443 ],\n",
      "        [0.19697303, 0.43141818, 0.5296637 , ..., 0.07205909,\n",
      "         0.8685205 , 0.84199315]]], dtype=float32)}, 'cifar_0': {'image': array([[[-0.5372549 , -0.6627451 , -0.60784316, ...,  0.23921573,\n",
      "          0.19215691,  0.16078436],\n",
      "        [-0.8745098 , -1.        , -0.85882354, ..., -0.03529412,\n",
      "         -0.06666666, -0.04313725],\n",
      "        [-0.8039216 , -0.8745098 , -0.6156863 , ..., -0.0745098 ,\n",
      "         -0.05882353, -0.14509803],\n",
      "        ...,\n",
      "        [ 0.6313726 ,  0.5764706 ,  0.5529412 , ...,  0.254902  ,\n",
      "         -0.56078434, -0.58431375],\n",
      "        [ 0.41176474,  0.35686278,  0.45882356, ...,  0.4431373 ,\n",
      "         -0.23921567, -0.3490196 ],\n",
      "        [ 0.38823533,  0.3176471 ,  0.4039216 , ...,  0.69411767,\n",
      "          0.18431377, -0.03529412]],\n",
      "\n",
      "       [[-0.5137255 , -0.6392157 , -0.62352943, ...,  0.03529418,\n",
      "         -0.01960784, -0.02745098],\n",
      "        [-0.84313726, -1.        , -0.9372549 , ..., -0.3098039 ,\n",
      "         -0.3490196 , -0.31764704],\n",
      "        [-0.8117647 , -0.94509804, -0.7882353 , ..., -0.34117645,\n",
      "         -0.34117645, -0.42745095],\n",
      "        ...,\n",
      "        [ 0.33333337,  0.20000005,  0.26274514, ...,  0.04313731,\n",
      "         -0.75686276, -0.73333335],\n",
      "        [ 0.09019613, -0.03529412,  0.12941182, ...,  0.16078436,\n",
      "         -0.5137255 , -0.58431375],\n",
      "        [ 0.12941182,  0.01176476,  0.11372554, ...,  0.4431373 ,\n",
      "         -0.0745098 , -0.27843136]],\n",
      "\n",
      "       [[-0.5058824 , -0.64705884, -0.6627451 , ..., -0.15294117,\n",
      "         -0.19999999, -0.19215685],\n",
      "        [-0.84313726, -1.        , -1.        , ..., -0.5686275 ,\n",
      "         -0.60784316, -0.5529412 ],\n",
      "        [-0.8352941 , -1.        , -0.9372549 , ..., -0.60784316,\n",
      "         -0.60784316, -0.67058825],\n",
      "        ...,\n",
      "        [-0.24705881, -0.73333335, -0.79607844, ..., -0.45098037,\n",
      "         -0.94509804, -0.84313726],\n",
      "        [-0.24705881, -0.67058825, -0.7647059 , ..., -0.26274508,\n",
      "         -0.73333335, -0.73333335],\n",
      "        [-0.09019607, -0.26274508, -0.31764704, ...,  0.09803927,\n",
      "         -0.34117645, -0.4352941 ]]], dtype=float32), 'label': 6, 'object_id': 0}, 'object_id': 0}\n",
      "[2025-07-11 17:25:30,972 hyrax.models.hyrax_autoencoder:WARNING] Found shape: torch.Size([7, 32, 32]) in data sample, using this to initialize model.\n",
      "[2025-07-11 17:25:30,975 hyrax.models.model_registry:INFO] Using criterion: torch.nn.CrossEntropyLoss with default arguments.\n",
      "[2025-07-11 17:25:30,976 hyrax.verbs.infer:INFO] data set has length 50000\n",
      "2025-07-11 17:25:30,976 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<hyrax.data_sets.dat': \n",
      "\t{'sampler': None, 'batch_size': 512, 'shuffle': False, 'pin_memory': False}\n",
      "[2025-07-11 17:25:30,994 hyrax.verbs.infer:INFO] Saving inference results at: /Users/drew/code/hyrax/docs/pre_executed/results/20250711-172523-infer-uETc\n",
      "[2025-07-11 17:25:31,190 hyrax.pytorch_ignite:INFO] Evaluating model on device: mps\n",
      "[2025-07-11 17:25:31,191 hyrax.pytorch_ignite:INFO] Total epochs: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f30e14613cf4fc9857aeab393673281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  1%|1         | 1/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-11 17:25:46,985 hyrax.pytorch_ignite:INFO] Total evaluation time: 15.79[s]\n",
      "[2025-07-11 17:25:47,042 hyrax.verbs.infer:INFO] Inference Complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hyrax.data_sets.inference_dataset.InferenceDataSet at 0x1588a7950>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae87db3e",
   "metadata": {},
   "source": [
    "# Things get not-so-great after this\n",
    "From here we really need to consider how we save state.\n",
    "We need to make sure that the datasets collected in ``DataProvider`` are copied to the config.\n",
    "We need to figure out how to recreate the current DataProvider from the config.\n",
    "And we need to figure out when to use the contents of the persisted config file vs.\n",
    "when to use the ``data`` attribute in the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d137a0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-11 16:53:49,256 hyrax.data_sets.inference_dataset:INFO] Using most recent results dir /Users/drew/code/hyrax/docs/pre_executed/results/20250711-165314-infer-RP-f for lookup. Use the [results] inference_dir config to set a directory or pass it to this verb.\n",
      "[2025-07-11 16:53:56,875 hyrax.verbs.umap:INFO] Saving UMAP results to /Users/drew/code/hyrax/docs/pre_executed/results/20250711-165356-umap-l_GX\n",
      "[2025-07-11 16:53:57,118 hyrax.verbs.umap:INFO] Fitting the UMAP\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "[2025-07-11 16:54:01,893 hyrax.verbs.umap:INFO] Saving fitted UMAP Reducer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7a5aeadcc94d2ab3331e1c545ab908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating lower dimensional representation using UMAP::   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-11 16:54:44,461 hyrax.verbs.umap:INFO] Finished transforming all data through UMAP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hyrax.data_sets.inference_dataset.InferenceDataSet at 0x3f91f26f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.umap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6efe6d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-11 17:10:05,902 hyrax.data_sets.inference_dataset:INFO] Using most recent results dir /Users/drew/code/hyrax/docs/pre_executed/results/20250711-165356-umap-l_GX for lookup. Use the [results] inference_dir config to set a directory or pass it to this verb.\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 60] Operation timed out>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/urllib/request.py:1344\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1344\u001b[39m     \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTransfer-encoding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/http/client.py:1338\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1337\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/http/client.py:1384\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1383\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/http/client.py:1333\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1333\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/http/client.py:1093\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1096\u001b[39m \n\u001b[32m   1097\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/http/client.py:1037\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/http/client.py:1472\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1470\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mConnect to a host on a given (SSL) port.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1472\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tunnel_host:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/http/client.py:1003\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1002\u001b[39m sys.audit(\u001b[33m\"\u001b[39m\u001b[33mhttp.client.connect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m.port)\n\u001b[32m-> \u001b[39m\u001b[32m1003\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/socket.py:865\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_errors:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m    866\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ExceptionGroup(\u001b[33m\"\u001b[39m\u001b[33mcreate_connection failed\u001b[39m\u001b[33m\"\u001b[39m, exceptions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/socket.py:850\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    849\u001b[39m     sock.bind(source_address)\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mTimeoutError\u001b[39m: [Errno 60] Operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mURLError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/verbs/visualize.py:75\u001b[39m, in \u001b[36mVisualize.run\u001b[39m\u001b[34m(self, input_dir, return_verb, **kwargs)\u001b[39m\n\u001b[32m     72\u001b[39m     fields += [\u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Get the umap data and put it in a kdtree for indexing.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28mself\u001b[39m.umap_results = \u001b[43mInferenceDataSet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverb\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mumap\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m available_fields = \u001b[38;5;28mself\u001b[39m.umap_results.metadata_fields()\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m fields.copy():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/data_sets/inference_dataset.py:97\u001b[39m, in \u001b[36mInferenceDataSet.__init__\u001b[39m\u001b[34m(self, config, results_dir, verb)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Disable cache preloading on this dataset because it will only be used for its metadata\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# TODO: May want to add some sort of metadata_only optional arg to dataset constructor\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m#       so we can opt-out of expensive dataset operations conditional on us only needing metadata\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m#       Alternatively this may be an opportunity for a metadata mixin sort of class structure where\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m#       we can bring up Only the metadata for a dataset, without constructing the whole thing.\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mself\u001b[39m._original_dataset_config[\u001b[33m\"\u001b[39m\u001b[33mdata_set\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mpreload_cache\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28mself\u001b[39m.original_dataset = \u001b[43msetup_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_original_dataset_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/pytorch_ignite.py:85\u001b[39m, in \u001b[36msetup_dataset\u001b[39m\u001b[34m(config, tensorboardx_logger)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Fetch data loader class specified in config and create an instance of it\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# data_set_cls = fetch_data_set_class(config)\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# data_set: HyraxDataset = data_set_cls(config)  # type: ignore[call-arg]\u001b[39;00m\n\u001b[32m     81\u001b[39m \n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# data_set.tensorboardx_logger = tensorboardx_logger\u001b[39;00m\n\u001b[32m     84\u001b[39m model_cls = fetch_model_class(config)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_setup_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboardx_logger\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/pytorch_ignite.py:56\u001b[39m, in \u001b[36m_setup_dataset\u001b[39m\u001b[34m(data_request, config, tensorboardx_logger)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_setup_dataset\u001b[39m(data_request, config, tensorboardx_logger):\n\u001b[32m     55\u001b[39m     data_provider = DataProvider(data_request, config)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[43mdata_provider\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m friendly_name \u001b[38;5;129;01min\u001b[39;00m data_provider.prepped_datasets:\n\u001b[32m     58\u001b[39m         data_provider.prepped_datasets[friendly_name].tensorboardx_logger = tensorboardx_logger\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/data_sets/data_provider.py:55\u001b[39m, in \u001b[36mDataProvider.prepare_datasets\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data_directory = dataset_definition.get(\u001b[33m'\u001b[39m\u001b[33mdata_directory\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     ds_instance = \u001b[43mDATA_SET_REGISTRY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mds_cls\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28mself\u001b[39m.prepped_datasets[friendly_name] = ds_instance\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ds_instance._metadata_table:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/hyrax/src/hyrax/data_sets/hyrax_cifar_data_set.py:27\u001b[39m, in \u001b[36mHyraxCifarBase.__init__\u001b[39m\u001b[34m(self, config, data_directory)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m.data_directory = data_directory \u001b[38;5;28;01mif\u001b[39;00m data_directory \u001b[38;5;28;01melse\u001b[39;00m config[\u001b[33m'\u001b[39m\u001b[33mgeneral\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mdata_dir\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     24\u001b[39m transform = transforms.Compose(\n\u001b[32m     25\u001b[39m     [transforms.ToTensor(), transforms.Normalize((\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m), (\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m))]\n\u001b[32m     26\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28mself\u001b[39m.cifar = \u001b[43mCIFAR10\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m metadata_table = Table(\n\u001b[32m     31\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m: np.array([\u001b[38;5;28mself\u001b[39m.cifar[index][\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.cifar))])}\n\u001b[32m     32\u001b[39m )\n\u001b[32m     33\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config, metadata_table)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/torchvision/datasets/cifar.py:66\u001b[39m, in \u001b[36mCIFAR10.__init__\u001b[39m\u001b[34m(self, root, train, transform, target_transform, download)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.train = train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_integrity():\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/torchvision/datasets/cifar.py:139\u001b[39m, in \u001b[36mCIFAR10.download\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_integrity():\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtgz_md5\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/torchvision/datasets/utils.py:391\u001b[39m, in \u001b[36mdownload_and_extract_archive\u001b[39m\u001b[34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[32m    389\u001b[39m     filename = os.path.basename(url)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    393\u001b[39m archive = os.path.join(download_root, filename)\n\u001b[32m    394\u001b[39m extract_archive(archive, extract_root, remove_finished)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/torchvision/datasets/utils.py:121\u001b[39m, in \u001b[36mdownload_url\u001b[39m\u001b[34m(url, root, filename, md5, max_redirect_hops)\u001b[39m\n\u001b[32m    118\u001b[39m     _download_file_from_remote_location(fpath, url)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# expand redirect chain if needed\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     url = \u001b[43m_get_redirect_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_hops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_redirect_hops\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     \u001b[38;5;66;03m# check if file is located on Google Drive\u001b[39;00m\n\u001b[32m    124\u001b[39m     file_id = _get_google_drive_file_id(url)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/site-packages/torchvision/datasets/utils.py:66\u001b[39m, in \u001b[36m_get_redirect_url\u001b[39m\u001b[34m(url, max_hops)\u001b[39m\n\u001b[32m     63\u001b[39m headers = {\u001b[33m\"\u001b[39m\u001b[33mMethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mHEAD\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m: USER_AGENT}\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_hops + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[32m     67\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m response.url == url \u001b[38;5;129;01mor\u001b[39;00m response.url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     68\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m url\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/urllib/request.py:215\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/urllib/request.py:515\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    512\u001b[39m     req = meth(req)\n\u001b[32m    514\u001b[39m sys.audit(\u001b[33m'\u001b[39m\u001b[33murllib.Request\u001b[39m\u001b[33m'\u001b[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[32m    518\u001b[39m meth_name = protocol+\u001b[33m\"\u001b[39m\u001b[33m_response\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/urllib/request.py:532\u001b[39m, in \u001b[36mOpenerDirector._open\u001b[39m\u001b[34m(self, req, data)\u001b[39m\n\u001b[32m    529\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    531\u001b[39m protocol = req.type\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/urllib/request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    491\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/urllib/request.py:1392\u001b[39m, in \u001b[36mHTTPSHandler.https_open\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/miniconda3/envs/hyrax/lib/python3.12/urllib/request.py:1347\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1344\u001b[39m         h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[32m   1345\u001b[39m                   encode_chunked=req.has_header(\u001b[33m'\u001b[39m\u001b[33mTransfer-encoding\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m   1346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[32m   1348\u001b[39m     r = h.getresponse()\n\u001b[32m   1349\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[31mURLError\u001b[39m: <urlopen error [Errno 60] Operation timed out>"
     ]
    }
   ],
   "source": [
    "h.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyrax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
